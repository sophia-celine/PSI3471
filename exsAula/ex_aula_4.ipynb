{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26fxjoVADscQ"
   },
   "source": [
    "Para abrir o notebook no Google Colab, altere o domínio `github.com` para `githubtocolab.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c04thLl8Dzh4"
   },
   "source": [
    "# PSI3471 - Aula de Exercícios 04 \n",
    "\n",
    "# MLP e o backpropagation\n",
    "\n",
    "Neste exercício vamos treinar uma rede MLP com o algoritmo backpropagation no modo mini-batch para o caso do XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VfzscWYDgio",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOpCJWjHEJII"
   },
   "source": [
    "Vamos gerar os dados de treinamento do problema do XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMhGEjZzNww7"
   },
   "outputs": [],
   "source": [
    "Nt = 500\n",
    "x = np.round(np.random.uniform(0, 1, (Nt, 2)), 0)\n",
    "d = 1 * (np.logical_xor(x[:, [0]], x[:, [1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD1k6Gf4iPL0"
   },
   "source": [
    "Vamos usar a sigmoide como função de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbSlc5ICSrZs"
   },
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "   return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5R8LPYEiY3U"
   },
   "source": [
    "A função abaixo implementa uma MLP de duas camadas com configuração 2-1 no modo de treinamento mini-batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Complete o código a seguir:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwadJsqLgYBw"
   },
   "outputs": [],
   "source": [
    "def redeMLP_21(x, d, eta, Nt, Nb, Ne, W01_1, W02_1, W01_2):\n",
    "    \"\"\"\n",
    "    J_MSE, W1_1, W2_1, W1_2 = redeMLP(x, d, Nn, eta, Nt, Nb, Ne, W0)\n",
    "    Saídas:\n",
    "    J_MSE: valor da função custo ao longo das épocas\n",
    "    W1_1: vetor de pesos do neurônio 1 da camada 1\n",
    "    W2_1: vetor de pesos do neurônio 2 da camada 1\n",
    "    W1_2: vetor de pesos do neurônio 1 da camada de saída\n",
    "    Entradas:\n",
    "    x: sinal de entrada\n",
    "    d: sinal desejado\n",
    "    eta: passo de adaptação\n",
    "    Nt: número de dados de treinamento\n",
    "    Nb: tamanho do mini-batch\n",
    "    Ne: número de épocas\n",
    "    W01_1: vetor de pesos do neurônio 1 da camada 1 (útlima iteração, inclui o bias)\n",
    "    W02_1: vetor de pesos do neurônio 2 da camada 1 (útlima iteração, inclui o bias)\n",
    "    W01_2: vetor de pesos o neurônio 1 da camada de saída (útlima iteração, inclui o bias)\n",
    "    \"\"\"\n",
    "\n",
    "       \n",
    "    # número de mini-batches por época\n",
    "    Nmb = int(np.floor(Nt / Nb))\n",
    "    \n",
    "    # inicialização dos pesos\n",
    "    W1_1 = W01_1.copy()\n",
    "    W2_1 = W02_1.copy()\n",
    "    W1_2 = W01_2.copy()\n",
    "    \n",
    "    # passo de adaptação dividido pelo tamanho do mini-batch\n",
    "    eta = eta / Nb\n",
    "\n",
    "    # inicialização do vetor que contém o valor da função custo\n",
    "    J_MSE = np.zeros((Ne, 1))\n",
    "\n",
    "    # Juntamos o vetor de entrada com o sinal desejado e inserimos\n",
    "    # uma coluna de uns para levar em conta o bias\n",
    "    Xd = np.hstack((np.ones((Nt, 1)), x, d))\n",
    "\n",
    "    # vetor de uns para o bias no mini-batch\n",
    "    b = np.ones((Nb, 1))\n",
    "\n",
    "    # for das épocas\n",
    "    for k in range(Ne): \n",
    "        np.random.shuffle(Xd)\n",
    "        X = Xd[:, 0 : 3]\n",
    "        d = Xd[:, [3]]\n",
    "\n",
    "        # for dos mini-batches\n",
    "        for l in range(Nmb):\n",
    "            dmb = d[l * Nb : (l + 1) * Nb].reshape(-1, 1)\n",
    "            X1mb = X[l * Nb : (l + 1) * Nb, :]\n",
    "            \n",
    "            # Cálculo Progressivo\n",
    "            # Neurônio 1 da camada 1\n",
    "            v1mb_1 = X1mb @ W1_1.T\n",
    "            y1mb_1 = sigmoid(v1mb_1)\n",
    "            dphi1_1 = y1mb_1 * (1 - y1mb_1)  # derivada da função sigmoide\n",
    "\n",
    "            # Neurônio 2 da camada 1\n",
    "            v2mb_1 = X1mb @ W2_1.T\n",
    "            y2mb_1 = sigmoid(v2mb_1)\n",
    "            dphi2_1 = y2mb_1 * (1 - y2mb_1) \n",
    "            \n",
    "            # Neurônio de saída\n",
    "            X2mb = np.hstack((b, y1mb_1, y2mb_1))\n",
    "            v1mb_2 = X2mb @ W1_2.T\n",
    "            y1mb_2 = sigmoid(v1mb_2)\n",
    "            dphi1_2 = y1mb_2 * (1 - y1mb_2) \n",
    "\n",
    "            # erro da última camada                \n",
    "            e1mb_2 = dmb - y1mb_2 \n",
    "\n",
    "            #############\n",
    "            # Complete o código\n",
    "            # cálculo dos gradientes locais\n",
    "            delta1_2 = \n",
    "            delta1_1 = \n",
    "            delta2_1 = \n",
    "            \n",
    "            # atualização dos pesos da camada de saída\n",
    "            W1_2 =  \n",
    "              \n",
    "            # atualização dos pesos da camada 1\n",
    "            W1_1 = \n",
    "            W2_1 = \n",
    "            #############\n",
    "\n",
    "            # guarda no vetor J_MSE a norma do vertor de erros de saída ao quadrado\n",
    "            J_MSE[k] = (J_MSE[k] + (np.linalg.norm(e1mb_2)) ** 2)\n",
    "\n",
    "        # cálculo do MSE (divide o valor acumulado pelo número de\n",
    "        # mini-batches x tamanho do batch x número de neurônios\n",
    "        # da camada de saída)        \n",
    "        J_MSE[k] = J_MSE[k] / (Nmb * Nb * 1)\n",
    "        \n",
    "        if k % 100 == 0:\n",
    "            print(f\"Época: {k}, MSE: {J_MSE[k]}\")\n",
    "          \n",
    "    return J_MSE, W1_1, W2_1, W1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651M3YyxDgis",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parâmetros da rede\n",
    "\n",
    "# passo de adaptação do algoritmo backpropagation\n",
    "eta = 0.9\n",
    "\n",
    "# Inicialização dos pesos\n",
    "W01_1 = 0.2 * np.random.rand(1, 3) - 0.01\n",
    "W02_1 = 0.2 * np.random.rand(1, 3) - 0.01\n",
    "W01_2 = 0.2 * np.random.rand(1, 3) - 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8-g2iTUOHQn"
   },
   "source": [
    "Treinamento da MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIVTvA4cDgis",
    "outputId": "71829ecf-5ab6-4189-bce2-3568972c0fd6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "\n",
    "# Tamanho do mini-batch\n",
    "Nb = 20\n",
    "\n",
    "# Número de épocas\n",
    "Ne = 5000\n",
    "\n",
    "(J_MSE, W1_1, W2_1, W1_2) = redeMLP_21(x, d, eta, Nt, Nb, Ne, W01_1, W02_1, W01_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "JuYm9li3Dgis",
    "outputId": "11ab3393-524d-4b16-f921-7a28cb53deb3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mostra a função custo ao longo das épocas\n",
    "\n",
    "plt.figure()\n",
    "J_MSEdB = 10 * np.log10(J_MSE)\n",
    "plt.plot(J_MSEdB, \"r\")\n",
    "plt.ylabel(\"J_MSE (dB)\")\n",
    "plt.grid(axis=\"x\", color=\"0.5\")\n",
    "plt.grid(axis=\"y\", color=\"0.5\")\n",
    "plt.xlabel(\"épocas\")\n",
    "plt.axis([0, Ne, min(J_MSEdB) - 1, max(J_MSEdB) + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-GpgjDuje88"
   },
   "source": [
    "Vamos gerar os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsEW69vxOhf7"
   },
   "outputs": [],
   "source": [
    "Nteste = 500\n",
    "xteste = np.round(np.random.uniform(0, 1, (Nteste, 2)), 0)\n",
    "dteste = 1 * (np.logical_xor(xteste[:, [0]], xteste[:, [1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svGefxTXjiz_"
   },
   "source": [
    "A função abaixo mantém os pesos fixos e faz apenas o cálculo progressivo para testar a rede treinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoorUOcLDgit",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def redeMLP_teste_21(x, d, W1_1, W2_1, W1_2, Nteste):\n",
    "    \"\"\"\n",
    "    J_MSE,y = redeMLP_teste_21(x, d, W1_1, W2_1, W1_2, Nn, Nteste)\n",
    "    Saídas:\n",
    "    J_MSE: valor da função custo no teste\n",
    "    y: saída da rede MLP\n",
    "    Entradas:\n",
    "    x: sinal de entrada\n",
    "    d: sinal desejado\n",
    "    W1_1: vetor de pesos do neurônio 1 da camada 1\n",
    "    W2_1: vetor de pesos do neurônio 2 da camada 1\n",
    "    W1_2: vetor de pesos do neurônio 1 da camada de saída\n",
    "    Nteste: número de dados de teste\n",
    "    \"\"\"\n",
    "   \n",
    "    # insere 1's por causa do bias    \n",
    "    x = np.hstack((np.ones((Nteste, 1)), x))\n",
    "\n",
    "\n",
    "    J_MSE = np.zeros((Nteste, 1))\n",
    "    y = np.zeros((Nteste, 1))\n",
    "    e = np.zeros((Nteste, 1))\n",
    "    b = 1\n",
    "\n",
    "    for n in range(Nteste):\n",
    "        X1 = x[n, :]\n",
    "\n",
    "        # cálculo progressivo com os pesos fixos da última época\n",
    "        v1_1 = X1 @ W1_1.T\n",
    "        y1_1 = sigmoid(v1_1)\n",
    "            \n",
    "        v2_1 = X1 @ W2_1.T\n",
    "        y2_1 = sigmoid(v2_1)\n",
    "\n",
    "        X2 = np.hstack((b, y1_1, y2_1))\n",
    "\n",
    "        v1_2 = X2 @ W1_2.T\n",
    "        y1_2 = sigmoid(v1_2)\n",
    "            \n",
    "            \n",
    "        y[n, :] = y1_2\n",
    "        e[n, :] = d[n, :] - y[n, :]\n",
    "        J_MSE[n] = (J_MSE[n] + (np.linalg.norm(e[n, :])) ** 2) / (1)\n",
    "\n",
    "    return J_MSE, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYRrmq4sd90v"
   },
   "source": [
    "Teste da rede MLP (apenas o cálculo progressivo com pesos da última época)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NumXYhyDgit",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(J_MSEteste, yteste) = redeMLP_teste_21(xteste, dteste, W1_1, W2_1, W1_2, Nteste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "GpyiJSXeDgiu",
    "outputId": "09aeb9aa-155e-42c8-f757-d87f9f988c6b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mostra a função custo com dados de teste em dB\n",
    "plt.figure()\n",
    "J_MSEtestedB = 10 * np.log10(J_MSEteste)\n",
    "plt.plot(J_MSEtestedB, \".r\")\n",
    "plt.ylabel(\"J_MSE\")\n",
    "plt.grid(axis=\"x\", color=\"0.5\")\n",
    "plt.grid(axis=\"y\", color=\"0.5\")\n",
    "plt.xlabel(\"iterações\")\n",
    "plt.axis([0, Nteste, min(J_MSEtestedB) - 1, max(J_MSEtestedB) + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B82O2P0GkEf0"
   },
   "source": [
    "Vamos calcular a taxa de erro do teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZV5VEJsCDgiu",
    "outputId": "868b8ae2-0077-4b4a-ad8f-bd1d1c149fcb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "yteste_dec = np.round(yteste, 0)  # arredonda a saída do teste\n",
    "\n",
    "Taxa_de_erro = np.sum(np.absolute(dteste - yteste_dec)) * 100 / (2 * Nteste)\n",
    "print(f\"Taxa de erro: {Taxa_de_erro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "Ge4OiH9Kf6cj",
    "outputId": "43411c8c-9369-4580-813a-340f1dc96505",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gera a curva de separação das duas regiões\n",
    "# Dados da curva de separação\n",
    "Nsep = 100\n",
    "x1S = np.linspace(-0.5, 1.5, Nsep).reshape(-1, 1)\n",
    "x2S = np.linspace(-0.5, 1.5, Nsep).reshape(-1, 1)\n",
    "\n",
    "# Gera pontos da grade\n",
    "xx1S, xx2S = np.meshgrid(x1S, x2S)\n",
    "xx1S = xx1S.reshape(-1, 1)\n",
    "xx2S = xx2S.reshape(-1, 1)\n",
    "\n",
    "# Gera arrays x e d (nesse caso, d é \"qualquer\", apenas usado\n",
    "# pois é entrada obrigatória da função redeMLP_teste_21()\n",
    "xgrid = np.hstack((xx1S, xx2S))\n",
    "Ngrid = len(xgrid)\n",
    "dgrid = np.zeros((Ngrid, 1))\n",
    "\n",
    "# Calcula saída para cada ponto da grade\n",
    "(J_MSEgrid, ygrid) = redeMLP_teste_21(xgrid, dgrid, W1_1, W2_1, W1_2, Ngrid)\n",
    "ygrid_dec = np.round(ygrid, 0)\n",
    "\n",
    "# Plota os pontos principais\n",
    "fig, ax2 = plt.subplots()\n",
    "for i in range(Nteste):\n",
    "    if dteste[i] == 1:\n",
    "        ax2.plot(xteste[i, 0], xteste[i, 1], \"xb\")\n",
    "    else:\n",
    "        ax2.plot(xteste[i, 0], xteste[i, 1], \"or\")\n",
    "\n",
    "# Plota pontos da grade com saída 0 (usa transparência alpha)        \n",
    "l0 = np.where(ygrid_dec == 0)[0]\n",
    "ax2.plot(xgrid[l0, 0], xgrid[l0, 1], \"r.\", alpha=0.1)\n",
    "\n",
    "# Plota pontos da grade com saída 1 (usa transparência alpha)\n",
    "l1 = np.where(ygrid_dec == 1)[0]\n",
    "ax2.plot(xgrid[l1, 0], xgrid[l1, 1], \"b.\", alpha=0.1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
