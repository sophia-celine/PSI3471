{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26fxjoVADscQ"
   },
   "source": [
    "Para abrir o notebook no Google Colab, altere o domínio `github.com` para `githubtocolab.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c04thLl8Dzh4"
   },
   "source": [
    "# PSI3471 - Aula de Exercícios 05 \n",
    "\n",
    "# MLP e o otimizador Adam\n",
    "\n",
    "Neste exercício vamos treinar uma rede MLP com o algorotimo backpropagation e comparar com o otimizador Adam no modo mini-batch para o caso das meias-luas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VfzscWYDgio",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOpCJWjHEJII"
   },
   "source": [
    "Vamos gerar os dados de treinamento do problema das meias-luas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxrHm1QrENqu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def meias_luas(NA, NB, r1, r2, r3):\n",
    "    \"\"\"\n",
    "    dados = meias_luas(NA,NB,r1,r2,r3)\n",
    "    NA: número de pontos da região A\n",
    "    NB: número de pontos da região B\n",
    "    r1, r2 e r3: dados das meias-luas\n",
    "    \"\"\"\n",
    "\n",
    "    # total de dados de treinamento\n",
    "    Nt = NA + NB\n",
    "\n",
    "    # dados das meia luas\n",
    "    rmin = r1 - r3 / 2\n",
    "    rmax = r1 + r3 / 2\n",
    "\n",
    "    # Pontos da Região A\n",
    "    a = np.pi * np.random.rand(NA, 1)\n",
    "    rxy = np.random.uniform(rmin, rmax, (NA, 1))\n",
    "    x1A = rxy * np.cos(a)\n",
    "    x2A = rxy * np.sin(a)\n",
    "    dA = np.ones((NA, 1))\n",
    "    pontosA = np.hstack((x1A, x2A, dA))\n",
    "\n",
    "    # Pontos da Região B\n",
    "    a = np.pi * np.random.rand(NB, 1)\n",
    "    rxy = np.random.uniform(rmin, rmax, (NB, 1))\n",
    "    x1B = rxy * np.cos(a) + r1\n",
    "    x2B = -rxy * np.sin(a) - r2\n",
    "    dB = -np.ones((NB, 1))\n",
    "    pontosB = np.hstack((x1B, x2B, dB))\n",
    "\n",
    "    # Concatenando e embaralhando os dados\n",
    "    dados = np.vstack((pontosA, pontosB))\n",
    "    np.random.shuffle(dados)\n",
    "\n",
    "    # Figura para mostrar os dados de treino\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(x1A, x2A, \".b\")\n",
    "    ax1.plot(x1B, x2B, \".r\")\n",
    "    plt.xlabel(\"x_1\")\n",
    "    plt.ylabel(\"x_2\")\n",
    "    plt.grid(axis=\"x\", color=\"0.5\")\n",
    "    plt.grid(axis=\"y\", color=\"0.5\")\n",
    "\n",
    "    return dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke2G_O0HESMp"
   },
   "source": [
    "Gera dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "A9ZHkepTDgiq",
    "outputId": "35eaab2c-34df-4145-f8c6-05db4cbb6d73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# número de pontos de treinamento da Região A\n",
    "NA = 500\n",
    "\n",
    "# número de pontos de treinamento da Região B\n",
    "NB = 500\n",
    "\n",
    "# número total de dados de treinamento\n",
    "Nt = NA + NB\n",
    "\n",
    "r1 = 10\n",
    "r3 = 6\n",
    "r2 = -4\n",
    "\n",
    "dados_treino = meias_luas(NA, NB, r1, r2, r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Myq3-VADDgir",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# organizando os dados para entrada da MLP\n",
    "\n",
    "# sinal de entrada\n",
    "x = dados_treino[:, [0, 1]]\n",
    "\n",
    "# sinal desejado\n",
    "d = dados_treino[:, [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAhy9A4ODgis",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def redeMLP_SGD(x, d, Nn, eta, Nt, Nb, Ne, W0):\n",
    "    \"\"\"\n",
    "    J_MSE, W = redeMLP_SGD(x, d, Nn, eta, Nt, Nb, Ne, W0)\n",
    "    Saídas:\n",
    "    J_MSE: valor da função custo ao longo das épocas\n",
    "    W: matriz de pesos (inclui o bias) da última época\n",
    "    Entradas:\n",
    "    x: sinal de entrada\n",
    "    d: sinal desejado\n",
    "    Nn: vetor que contém o número de neurônios em cada camada,\n",
    "        mas na posição 0, contém o número de entradas da rede\n",
    "    eta: passo de adaptação\n",
    "    Nt: número de dados de treinamento\n",
    "    Nb: tamanho do mini-batch\n",
    "    Ne: número de épocas\n",
    "    W0: matriz de pesos (inclui o bias)\n",
    "    \"\"\"\n",
    "\n",
    "    # número de camadas (desconta-se 1, pois na posição 0\n",
    "    # o vetor Nn contém o número de entradas da rede)\n",
    "    L = len(Nn) - 1\n",
    "\n",
    "    # número de neurônios da camada de saída\n",
    "    NL = Nn[L]\n",
    "\n",
    "    # número de mini-batches por época\n",
    "    Nmb = int(np.floor(Nt / Nb))\n",
    "\n",
    "    # inicialização da matriz W\n",
    "    W = W0.copy()\n",
    "\n",
    "    # inicialização da matriz de gradientes\n",
    "    Delta = 0 * W\n",
    "\n",
    "    # passo de adaptação dividido pelo tamanho do mini-batch\n",
    "    eta = eta / Nb\n",
    "\n",
    "    # inicialização do vetor que contém o valor da função custo\n",
    "    J_MSE = np.zeros((Ne, 1))\n",
    "\n",
    "    # Juntamos o vetor de entrada com o sinal desejado e inserimos\n",
    "    # uma coluna de uns para levar em conta o bias\n",
    "    Xd = np.hstack((np.ones((Nt, 1)), x, d))\n",
    "\n",
    "    # vetor de uns para o bias no mini-batch\n",
    "    b = np.ones((Nb, 1))\n",
    "\n",
    "    # for das épocas\n",
    "    for k in range(Ne):\n",
    "        np.random.shuffle(Xd)\n",
    "        X = Xd[:, 0 : Nn[0] + 1]\n",
    "        d = Xd[:, [Nn[0] + 1]]\n",
    "\n",
    "        # for dos mini-batches\n",
    "        for l in range(Nmb):\n",
    "            dmb = d[l * Nb : (l + 1) * Nb].reshape(-1, 1)\n",
    "            Xmb = X[l * Nb : (l + 1) * Nb, :]\n",
    "            dphi = np.zeros((Nb, max(Nn), L))\n",
    "            XXmb = np.zeros((Nb, max(Nn) + 1, L))\n",
    "\n",
    "            # for das camadas no cálculo progressivo\n",
    "            for j in range(L):\n",
    "                vmb = Xmb @ W[0 : Nn[j + 1], 0 : Nn[j] + 1, j].T\n",
    "                ymb = np.tanh(vmb)\n",
    "                dphi[:, 0 : Nn[j + 1], j] = 1 - ymb**2\n",
    "\n",
    "                # guarda a entrada da camada atual para atualização dos pesos\n",
    "                XXmb[:, 0 : Nn[j] + 1, j] = Xmb\n",
    "\n",
    "                # entrada da acmada seguinte formada por um vertor de uns por\n",
    "                # causa dos bias e o vetor que corresponde à saída da camada atual\n",
    "                Xmb = np.hstack((b, ymb))\n",
    "\n",
    "            # erro da última camada\n",
    "            emb = dmb - ymb\n",
    "\n",
    "            # for das camadas no cálculo regressivo para atualização dos pesos\n",
    "            for j in range(L, 0, -1):\n",
    "                \n",
    "                # Cálculo dos gradientes locais e da matriz de gradientes\n",
    "                if j == L:\n",
    "                    delta = emb * dphi[:, 0 : Nn[j], j - 1]\n",
    "                    Delta[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] = (\n",
    "                        delta.T @ XXmb[:, 0 : Nn[j - 1] + 1, j - 1]\n",
    "                    )\n",
    "                else:\n",
    "                    delta_aux = delta @ W[0 : Nn[j + 1], 1 : Nn[j] + 1, j]\n",
    "                    delta = dphi[:, 0 : Nn[j], j - 1] * delta_aux\n",
    "                    Delta[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] = (\n",
    "                        delta.T @ XXmb[:, 0 : Nn[j - 1] + 1, j - 1]\n",
    "                    )\n",
    "\n",
    "            for j in range(L, 0, -1):\n",
    "                \n",
    "                # Atualização dos pesos e biases\n",
    "                W[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] = (\n",
    "                    W[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1]\n",
    "                    + eta * Delta[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1]\n",
    "                )\n",
    "\n",
    "            # guarda no vetor J_MSE a norma do vertor de erros de saída ao quadrado\n",
    "            J_MSE[k] = J_MSE[k] + (np.linalg.norm(emb)) ** 2\n",
    "\n",
    "        # cálculo do MSE (divide o valor acumulado pelo número de\n",
    "        # mini-batches x tamanho do batch x número de neurônios\n",
    "        # da camada de saída)\n",
    "        J_MSE[k] = J_MSE[k] / (Nmb * Nb * NL)\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            print(f\"SGD - Época: {k}\", f\"MSE: {J_MSE[k]}\")\n",
    "    return J_MSE, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Complete o código a seguir:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MtsOQo9Uxr9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def redeMLP_Adam(x, d, Nn, eta, Nt, Nb, Ne, W0, beta1, beta2, epsilon):\n",
    "    \"\"\"\n",
    "    J_MSE, W = redeMLP(x, d, Nn, eta, Nt, Nb, Ne, W0, beta1, beta2, epsilon)\n",
    "    Saídas:\n",
    "    J_MSE: valor da função custo ao longo das épocas\n",
    "    W: matriz de pesos (inclui o bias) da última época\n",
    "    Entradas:\n",
    "    x: sinal de entrada\n",
    "    d: sinal desejado\n",
    "    Nn: vetor que contém o número de neurônios em cada camada,\n",
    "        mas na posição 0, contém o número de entradas da rede\n",
    "    eta: passo de adaptação\n",
    "    Nt: número de dados de treinamento\n",
    "    Nb: tamanho do mini-batch\n",
    "    Ne: número de épocas\n",
    "    W0: matriz de pesos (inclui o bias)\n",
    "    beta1, beta2: fatores de esquecimento do otimizador Adam (e.g., beta1 = beta2 = 0.99)\n",
    "    epsilon: constante para evitar divisão por zero no Adam (e.g. epsilon = 1e-4)\n",
    "    \"\"\"\n",
    "\n",
    "    # número de camadas (desconta-se 1, pois na posição 0 o vetor Nn contém o número de entradas da rede)\n",
    "    L = len(Nn) - 1\n",
    "\n",
    "    # número de neurônios da camada de saída\n",
    "    NL = Nn[L]\n",
    "\n",
    "    # número de mini-batches por época\n",
    "    Nmb = int(np.floor(Nt / Nb))\n",
    "\n",
    "    # inicialização da matriz W\n",
    "    W = W0.copy()\n",
    "\n",
    "    # inicialização da matriz de gradientes\n",
    "    Delta = 0 * W\n",
    "\n",
    "    # passo de adaptação dividido pelo tamanho do mini-batch\n",
    "    eta = eta / Nb\n",
    "\n",
    "    # inicialização do vetor que contém o valor da função custo\n",
    "    J_MSE = np.zeros((Ne, 1))\n",
    "\n",
    "    # Juntamos o vetor de entrada com o sinal desejado e inserimos\n",
    "    # uma coluna de uns para levar em conta o bias\n",
    "    Xd = np.hstack((np.ones((Nt, 1)), x, d))\n",
    "\n",
    "    # vetor de uns para o bias no mini-batch\n",
    "    b = np.ones((Nb, 1))\n",
    "\n",
    "    # inicialização das matrizes S e V\n",
    "    S = 0 * W0\n",
    "    V = 0 * W0\n",
    "    m = 1  # contador de iteração\n",
    "\n",
    "    # for das épocas\n",
    "    for k in range(Ne):\n",
    "        np.random.shuffle(Xd)\n",
    "        X = Xd[:, 0 : Nn[0] + 1]\n",
    "        d = Xd[:, [Nn[0] + 1]]\n",
    "\n",
    "        # for dos mini-batches\n",
    "        for l in range(Nmb):\n",
    "            dmb = d[l * Nb : (l + 1) * Nb].reshape(-1, 1)\n",
    "            Xmb = X[l * Nb : (l + 1) * Nb, :]\n",
    "            dphi = np.zeros((Nb, max(Nn), L))\n",
    "            XXmb = np.zeros((Nb, max(Nn) + 1, L))\n",
    "\n",
    "            # for das camadas no cálculo progressivo\n",
    "            for j in range(L):\n",
    "                vmb = Xmb @ W[0 : Nn[j + 1], 0 : Nn[j] + 1, j].T\n",
    "                ymb = np.tanh(vmb)\n",
    "                dphi[:, 0 : Nn[j + 1], j] = 1 - ymb**2\n",
    "\n",
    "                # guarda a entrada da camada atual para atualização dos pesos\n",
    "                XXmb[:, 0 : Nn[j] + 1, j] = Xmb\n",
    "\n",
    "                # entrada da acmada seguinte formada por um vertor de uns por\n",
    "                # causa dos bias e o vetor que corresponde à saída da camada atual\n",
    "                Xmb = np.hstack((b, ymb))\n",
    "\n",
    "            # erro da última camada\n",
    "            emb = dmb - ymb\n",
    "\n",
    "            # for das camadas no cálculo regressivo para atualização dos pesos\n",
    "            for j in range(L, 0, -1):\n",
    "                \n",
    "                # Cálculo dos gradientes locais e da matriz de gradientes\n",
    "                if j == L:\n",
    "                    delta = emb * dphi[:, 0 : Nn[j], j - 1]\n",
    "                    Delta[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] = (\n",
    "                        delta.T @ XXmb[:, 0 : Nn[j - 1] + 1, j - 1]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    delta_aux = delta @ W[0 : Nn[j + 1], 1 : Nn[j] + 1, j]\n",
    "                    delta = dphi[:, 0 : Nn[j], j - 1] * delta_aux\n",
    "                    Delta[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] = (\n",
    "                        delta.T @ XXmb[:, 0 : Nn[j - 1] + 1, j - 1]\n",
    "                    )\n",
    "\n",
    "            for j in range(L, 0, -1):\n",
    "                \n",
    "                # Atualização dos pesos e biases\n",
    "                #############\n",
    "                #V[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] =                \n",
    "                #S[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] =\n",
    "\n",
    "                #Vb = \n",
    "\n",
    "                #Sb = \n",
    "\n",
    "                #W[0 : Nn[j], 0 : Nn[j - 1] + 1, j - 1] =\n",
    "                #############\n",
    "\n",
    "            m = m + 1  # incrementa o número da iteração\n",
    "\n",
    "            # guarda no vetor J_MSE a norma do vertor de erros de saída ao quadrado\n",
    "            J_MSE[k] = J_MSE[k] + (np.linalg.norm(emb)) ** 2\n",
    "\n",
    "        # cálculo do MSE (divide o valor acumulado pelo número de\n",
    "        # mini-batches x tamanho do batch x número de neurônios\n",
    "        # da camada de saída)\n",
    "        J_MSE[k] = J_MSE[k] / (Nmb * Nb * NL)\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            print(f\"Adam - Época: {k}\", f\"MSE: {J_MSE[k]}\")\n",
    "    return J_MSE, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651M3YyxDgis",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parâmetros da rede\n",
    "\n",
    "# passo de adaptação da rede MLP\n",
    "eta = 0.5\n",
    "\n",
    "# número de entradas da rede\n",
    "N0 = 2\n",
    "\n",
    "# vetor que contém o número de neurônios em cada camada,\n",
    "# mas na posição 0, contém o número de entradas da rede\n",
    "Nn = [N0, 3, 5, 5, 2, 1]\n",
    "\n",
    "# número de camadas\n",
    "L = len(Nn) - 1\n",
    "\n",
    "# Inicialização dos pesos\n",
    "W0 = 0.02 * np.random.rand(np.max(Nn), np.max(Nn) + 1, L) - 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8-g2iTUOHQn"
   },
   "source": [
    "Treinamento da MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIVTvA4cDgis",
    "outputId": "265f96a5-2a8a-4769-ab56-4d8d60857d5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "\n",
    "# Tamanho do mini-batch\n",
    "Nb = 50\n",
    "\n",
    "# Número de épocas\n",
    "Ne = 5000\n",
    "\n",
    "# Hiperparâmetros do Adam\n",
    "beta1 = 0.99\n",
    "beta2 = 0.99\n",
    "epsilon = 1e-4\n",
    "\n",
    "# Treinamento com o backpropagation\n",
    "(J_MSE_SGD, W_SGD) = redeMLP_SGD(x, d, Nn, eta, Nt, Nb, Ne, W0)\n",
    "\n",
    "# Treinamento com o otimizador Adam\n",
    "(J_MSE_Adam, W_Adam) = redeMLP_Adam(\n",
    "    x, d, Nn, eta, Nt, Nb, Ne, W0, beta1, beta2, epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "JuYm9li3Dgis",
    "outputId": "984f98ab-83f7-4aaa-d969-b6590aed60d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mostra a função custo ao longo das épocas\n",
    "\n",
    "plt.figure()\n",
    "J_MSEdB_SGD = 10 * np.log10(J_MSE_SGD)\n",
    "J_MSEdB_Adam = 10 * np.log10(J_MSE_Adam)\n",
    "plt.plot(J_MSEdB_SGD, \"r\", label=\"backpropagation\")\n",
    "plt.plot(J_MSEdB_Adam, \"b\", label=\"Adam\")\n",
    "plt.ylabel(\"J_MSE (dB)\")\n",
    "plt.grid(axis=\"x\", color=\"0.5\")\n",
    "plt.grid(axis=\"y\", color=\"0.5\")\n",
    "plt.xlabel(\"épocas\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "aJl7-f3KDgit",
    "outputId": "1b7a248f-b8c2-4d7c-f28b-83d92b39119a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dados de teste\n",
    "NAt = 1000\n",
    "NBt = 1000\n",
    "Nteste = NAt + NBt\n",
    "\n",
    "dados_teste = meias_luas(NAt, NBt, r1, r2, r3)\n",
    "\n",
    "# entrada\n",
    "xteste = dados_teste[:, 0:2]  # sinal de entrada\n",
    "\n",
    "# sinal desejado\n",
    "dteste = dados_teste[:, 2].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoorUOcLDgit",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def redeMLP_teste(x, d, W, Nn, Nteste):\n",
    "    \"\"\"\n",
    "    J_MSE,y = redeMLP_teste(x, d, W, Nn, Nteste)\n",
    "    Saídas:\n",
    "    J_MSE: valor da função custo no teste\n",
    "    y: saída da rede MLP\n",
    "    Entradas:\n",
    "    x: sinal de entrada\n",
    "    d: sinal desejado\n",
    "    W: matriz de pesos (inclui o bias) da última época\n",
    "    Nn: vetor que contém o número de neurônios em cada camada,\n",
    "        mas na posição 0, contém o número de entradas da rede\n",
    "    Nteste: número de dados de teste\n",
    "    \"\"\"\n",
    "\n",
    "    # vetor de 1's \n",
    "    b = np.ones((Nteste, 1))\n",
    "\n",
    "    # inseri 1's ao vetor de entrada\n",
    "    x = np.hstack((b, x))\n",
    "    \n",
    "    # número de camadas (desconta-se 1, pois na posição 0 o vetor\n",
    "    # Nn contém o número de entradas da rede)\n",
    "    L = len(Nn) - 1\n",
    "\n",
    "    # número de neurônios da camada de saída\n",
    "    NL = Nn[L]\n",
    "\n",
    "    J_MSE = np.zeros((Nteste, 1))\n",
    "    y = np.zeros((Nteste, NL))\n",
    "    e = np.zeros((Nteste, NL))\n",
    "    b = 1\n",
    "\n",
    "    for n in range(Nteste):\n",
    "        X = x[n, :]\n",
    "\n",
    "        # cálculo progressivo com os pesos fixos da última época\n",
    "        for j in range(L):\n",
    "            v = X @ W[0 : Nn[j + 1], 0 : Nn[j] + 1, j].T\n",
    "            yc = np.tanh(v)\n",
    "            X = np.hstack((b, yc))\n",
    "        y[n, :] = yc\n",
    "        e[n, :] = d[n, :] - y[n, :]\n",
    "        J_MSE[n] = (J_MSE[n] + (np.linalg.norm(e[n, :])) ** 2) / (Nn[L])\n",
    "\n",
    "    return J_MSE, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYRrmq4sd90v"
   },
   "source": [
    "Teste da rede MLP (apenas o cálculo progressivo com pesos da última época)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NumXYhyDgit",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# teste com a MLP treinada com o backpropagation\n",
    "(J_MSEteste_SGD, yteste_SGD) = redeMLP_teste(xteste, dteste, W_SGD, Nn, Nteste)\n",
    "\n",
    "# teste com a MLP treinada com o otimizador Adam\n",
    "(J_MSEteste_Adam, yteste_Adam) = redeMLP_teste(xteste, dteste, W_Adam, Nn, Nteste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "RyZrP186rsmY",
    "outputId": "b57dbabe-bc6d-41cd-fddd-0c77739bd539",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gera a curva de separação das duas regiões\n",
    "# Dados da curva de separação\n",
    "Nsep = 100\n",
    "x1S = np.linspace(-15, 25, Nsep).reshape(-1, 1)\n",
    "x2S = np.linspace(-10, 15, Nsep).reshape(-1, 1)\n",
    "\n",
    "# Gera pontos da grade\n",
    "xx1S, xx2S = np.meshgrid(x1S, x2S)\n",
    "xx1S = xx1S.reshape(-1, 1)\n",
    "xx2S = xx2S.reshape(-1, 1)\n",
    "\n",
    "# Gera arrays x e d (nesse caso, d é \"qualquer\", apenas usado\n",
    "# pois é entrada obrigatória da função redeMLP_teste()\n",
    "xgrid = np.hstack((xx1S, xx2S))\n",
    "Ngrid = len(xgrid)\n",
    "dgrid = np.zeros((Ngrid, 1))\n",
    "\n",
    "# Calcula saída para cada ponto da grade\n",
    "(J_MSEgrid_SGD, ygrid_SGD) = redeMLP_teste(xgrid, dgrid, W_SGD, Nn, Ngrid)\n",
    "(J_MSEgrid_Adam, ygrid_Adam) = redeMLP_teste(xgrid, dgrid, W_Adam, Nn, Ngrid)\n",
    "\n",
    "ygrid_dec_SGD = np.sign(ygrid_SGD)\n",
    "ygrid_dec_Adam = np.sign(ygrid_Adam)\n",
    "\n",
    "# Plota os pontos principais - SGD\n",
    "fig, ax2 = plt.subplots()\n",
    "for i in range(Nteste):\n",
    "    if dteste[i] == 1:\n",
    "        ax2.plot(xteste[i, 0], xteste[i, 1], \".b\")\n",
    "    else:\n",
    "        ax2.plot(xteste[i, 0], xteste[i, 1], \".r\")\n",
    "\n",
    "# Plota pontos da grade com saída 0 (usa transparência alpha)\n",
    "l0 = np.where(ygrid_dec_SGD == -1)[0]\n",
    "ax2.plot(xgrid[l0, 0], xgrid[l0, 1], \"r.\", alpha=0.1)\n",
    "\n",
    "# Plota pontos da grade com saída 1 (usa transparência alpha)\n",
    "l1 = np.where(ygrid_dec_SGD == 1)[0]\n",
    "ax2.plot(xgrid[l1, 0], xgrid[l1, 1], \"b.\", alpha=0.1)\n",
    "plt.title(\"SGD\")\n",
    "\n",
    "# Plota os pontos principais - Adam\n",
    "fig, ax3 = plt.subplots()\n",
    "for i in range(Nteste):\n",
    "    if dteste[i] == 1:\n",
    "        ax3.plot(xteste[i, 0], xteste[i, 1], \".b\")\n",
    "    else:\n",
    "        ax3.plot(xteste[i, 0], xteste[i, 1], \".r\")\n",
    "\n",
    "# Plota pontos da grade com saída 0 (usa transparência alpha)\n",
    "l0 = np.where(ygrid_dec_Adam == -1)[0]\n",
    "ax3.plot(xgrid[l0, 0], xgrid[l0, 1], \"r.\", alpha=0.1)\n",
    "\n",
    "# Plota pontos da grade com saída 1 (usa transparência alpha)\n",
    "l1 = np.where(ygrid_dec_Adam == 1)[0]\n",
    "ax3.plot(xgrid[l1, 0], xgrid[l1, 1], \"b.\", alpha=0.1)\n",
    "plt.title(\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNXDttjlxDMn"
   },
   "source": [
    "Calula a taxa de erro em cada caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ST3p4v53w6FO",
    "outputId": "d898f13f-c0a5-4d5e-9a08-7f6efc5e1754",
    "tags": []
   },
   "outputs": [],
   "source": [
    "yteste_dec_SGD = np.sign(yteste_SGD)\n",
    "yteste_dec_Adam = np.sign(yteste_Adam)\n",
    "\n",
    "Taxa_de_erro_SGD = np.sum(np.absolute(dteste - yteste_dec_SGD)) * 100 / (2 * Nteste)\n",
    "Taxa_de_erro_Adam = np.sum(np.absolute(dteste - yteste_dec_Adam)) * 100 / (2 * Nteste)\n",
    "\n",
    "print(f\"Taxa de erro SGD: {Taxa_de_erro_SGD}\")\n",
    "print(f\"Taxa de erro Adam: {Taxa_de_erro_Adam}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
